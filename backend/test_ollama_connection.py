#!/usr/bin/env python3
"""
Script rÃ¡pido para probar la conexiÃ³n con Ollama
"""

import requests
import json

print("ğŸ” Probando conexiÃ³n con Ollama...")

# Test 1: Verificar que Ollama estÃ¡ corriendo
try:
    response = requests.get("http://localhost:11434/api/tags", timeout=5)
    print(f"âœ… Ollama estÃ¡ corriendo (status: {response.status_code})")
    
    models = response.json().get('models', [])
    print(f"\nğŸ“¦ Modelos disponibles ({len(models)}):")
    for model in models:
        print(f"   - {model.get('name', 'unknown')}")
    
except requests.exceptions.ConnectionError:
    print("âŒ ERROR: No se puede conectar con Ollama")
    print("ğŸ’¡ SoluciÃ³n: Ejecuta 'ollama serve' en otra terminal")
    exit(1)
except Exception as e:
    print(f"âŒ ERROR: {e}")
    exit(1)

# Test 2: Probar generaciÃ³n simple
print("\nğŸ§ª Probando generaciÃ³n de texto...")
try:
    payload = {
        "model": "llama3.2:3b",
        "prompt": "Di 'Hola' en espaÃ±ol.",
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 50
        }
    }
    
    print(f"ğŸ“¤ Enviando: {payload['prompt']}")
    
    response = requests.post(
        "http://localhost:11434/api/generate",
        json=payload,
        timeout=30
    )
    
    if response.status_code == 200:
        result = response.json()
        print(f"âœ… Respuesta recibida:")
        print(f"   {result.get('response', 'Sin respuesta')}")
    else:
        print(f"âŒ Error: Status {response.status_code}")
        print(f"   {response.text}")
        
except Exception as e:
    print(f"âŒ ERROR en generaciÃ³n: {e}")
    exit(1)

print("\nâœ… Ollama funciona correctamente!")
print("ğŸ’¡ Ahora puedes probar el chat en el frontend")
